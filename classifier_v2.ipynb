{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Chord classifier\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "Xt-8qB58u2_y"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Packages installation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
    "!bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local"
   ],
   "metadata": {
    "id": "2SS7bMiAyRSX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.path.append('/usr/local/lib/python3.7/site-packages/')"
   ],
   "metadata": {
    "id": "8rhj8TkuyXRz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!conda install -c conda-forge ffmpeg libsndfile pandas numpy librosa matplotlib pytorch torchvision seaborn"
   ],
   "metadata": {
    "id": "SkbLGjSLu2_0"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%pip install spleeter"
   ],
   "metadata": {
    "id": "3v-Uf9rZu2_1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%pip install pandas numpy librosa matplotlib torch torchvision seaborn"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-QVyZk6y0nc_",
    "outputId": "26d5fd8c-e9b0-47a2-dc4c-f219ac7c3fd9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%pip install ray"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mb4P8XkEnmq7",
    "outputId": "23be590c-d72c-4527-ea7a-bbb3e308e07a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The purpose of preprocessing step, which is crucial for training the model, is to convert each chord represented as `wav` file into   spectrograms.\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "uMqj0zz2u2_1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ],
   "metadata": {
    "id": "OUhOhQU6u2_1"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Google Colab file management (no need to run outside Colab)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For usage of data in Google Colab, I mount my Google Drive and then copy files from it to the local instance to make processing faster."
   ],
   "metadata": {
    "collapsed": false,
    "id": "OPD9C4CRu2_1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "%cd /gdrive"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U0foJ3AMw0tN",
    "outputId": "121f6b98-6415-4876-f697-93ca69c255f6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "os.chdir(\"/content\")"
   ],
   "metadata": {
    "id": "EZFpDsTJxRpS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!cp -R \"/gdrive/My Drive/chord_classifier/data\" \"data\""
   ],
   "metadata": {
    "id": "z8E-C9mMpUXe"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data exploration\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "yGM2NlCju2_2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_dir = \"data\"\n",
    "preprocessed_train_data_dir = Path(os.path.join(data_dir, \"preprocessed\", \"train\"))\n",
    "preprocessed_test_data_dir = Path(os.path.join(data_dir, \"preprocessed\", \"test\"))\n",
    "pretrained_models_dir = Path(\"pretrained_models\")\n",
    "preprocessed_train_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "preprocessed_test_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "pretrained_models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_data_dir = data_dir + \"/Training\"\n",
    "test_data_dir = data_dir + \"/Test\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Show the number of chord recordings available for training and testing"
   ],
   "metadata": {
    "collapsed": false,
    "id": "ASY83Mqqu2_2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_chord_counts(data_dir):\n",
    "    chord_counts = {}\n",
    "\n",
    "    for chord in os.listdir(data_dir):\n",
    "        chord_path = os.path.join(data_dir, chord)\n",
    "        if os.path.isdir(chord_path):\n",
    "            chord_counts[chord] = len(os.listdir(chord_path))\n",
    "\n",
    "    return chord_counts"
   ],
   "metadata": {
    "id": "9vEb-JWvu2_2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Training data chord count {get_chord_counts(train_data_dir)}\")\n",
    "print(f\"Testing data chord count {get_chord_counts(test_data_dir)}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "imX0urefu2_2",
    "outputId": "aef1ff60-e468-4c13-c2b7-1156d5673f34"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Show the duration of sounds of chords. It ranges from 1.12 to 16.34 seconds with median about 5 seconds."
   ],
   "metadata": {
    "collapsed": false,
    "id": "Xm-YKrM1u2_3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "durations = []\n",
    "\n",
    "for chord in os.listdir(train_data_dir):\n",
    "    chord_path = os.path.join(train_data_dir, chord)\n",
    "    if os.path.isdir(chord_path):\n",
    "        for file in os.listdir(chord_path):\n",
    "            file_path = os.path.join(chord_path, file)\n",
    "            y, sr = librosa.load(file_path, sr=None)\n",
    "            durations.append(librosa.get_duration(y=y, sr=sr))\n",
    "\n",
    "durations_df = pd.DataFrame(durations, columns=['duration'])\n",
    "durations_df.describe()"
   ],
   "metadata": {
    "id": "PWs-EDGCu2_3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A little trick to create an image from a spectrogram with matplotlib and then save it."
   ],
   "metadata": {
    "collapsed": false,
    "id": "pWGyYICru2_3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_spectrogram(audio_input, sr=None, save_path=None):\n",
    "    # skip already existing sprectrogram\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"Spectrogram  {save_path} already exists. Skipping.\")\n",
    "        return\n",
    "\n",
    "    # Load the audio file if a path is provided, else use the provided audio data\n",
    "    if isinstance(audio_input, str):\n",
    "        y, sr = librosa.load(audio_input, sr=None)\n",
    "    else:\n",
    "        y = audio_input\n",
    "        if sr is None:\n",
    "            raise ValueError(\"Sampling rate must be provided with audio data\")\n",
    "\n",
    "    print(f\"Creating spectrogram {save_path}\")\n",
    "\n",
    "    # Generate the Mel spectrogram\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "    # Plot and save the spectrogram\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel')\n",
    "    plt.axis('off')\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', pad_inches=0, format=\"jpg\")\n",
    "    plt.close()\n",
    "\n",
    "    return S_dB\n"
   ],
   "metadata": {
    "id": "OcNPyjwhu2_3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def convert_chords_to_spectrograms(source_dir, destination_dir, durations=[0.4, 0.5, 0.6]):\n",
    "    for chord in os.listdir(source_dir):\n",
    "        chord_path = os.path.join(source_dir, chord)\n",
    "        preprocessed_chord_path = os.path.join(destination_dir, chord)\n",
    "\n",
    "        if not os.path.exists(preprocessed_chord_path):\n",
    "            os.makedirs(preprocessed_chord_path)\n",
    "\n",
    "        if os.path.isdir(chord_path):\n",
    "            for file in os.listdir(chord_path):\n",
    "                file_path = os.path.join(chord_path, file)\n",
    "                y, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "                for beat_duration in durations:\n",
    "                    samples_per_beat = int(beat_duration * sr)\n",
    "\n",
    "                    for i in range(0, len(y), samples_per_beat):\n",
    "                        end_frame = i + samples_per_beat\n",
    "                        if end_frame > len(y):\n",
    "                            end_frame = len(y)  # Adjust the end frame for the last segment\n",
    "\n",
    "                        segment = y[i:end_frame]\n",
    "                        duration_ms = int(beat_duration * 1000)  # Convert to milliseconds for filename\n",
    "                        save_path = os.path.join(preprocessed_chord_path, f\"{os.path.splitext(file)[0]}_{i // samples_per_beat}_{duration_ms}ms.jpg\")\n",
    "                        # Create spectrogram from a segment and save it to disk\n",
    "                        create_spectrogram(segment, sr, save_path)\n",
    "\n"
   ],
   "metadata": {
    "id": "Xa6Jel3ru2_3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert train and test data. This is going to take a while.\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "_oMlp5eSu2_3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "convert_chords_to_spectrograms(train_data_dir, preprocessed_train_data_dir)\n",
    "convert_chords_to_spectrograms(test_data_dir, preprocessed_test_data_dir)"
   ],
   "metadata": {
    "id": "oakTOTEiu2_4"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check the number of generated files."
   ],
   "metadata": {
    "collapsed": false,
    "id": "FnAdETfyu2_4"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Training data chord count {get_chord_counts(preprocessed_train_data_dir)}\")\n",
    "print(f\"Testing data chord count {get_chord_counts(preprocessed_test_data_dir)}\")"
   ],
   "metadata": {
    "id": "O9NI_yORu2_4"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "An example of a spectrogram"
   ],
   "metadata": {
    "collapsed": false,
    "id": "lkzRMMoXu2_4"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "image_path = os.path.join(preprocessed_train_data_dir, 'Am', 'Am_acousticguitar_Mari_1_0.jpg')\n",
    "\n",
    "img = mpimg.imread(image_path)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "rT2SeEOTu2_4"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Track splitting\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "rv762AGOu2_4"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "songs_dir = \"songs\"\n",
    "separated_songs_dir = os.path.join(songs_dir, \"separated\")\n",
    "song_file_name = \"Pet-Sematary\"\n",
    "song_path = os.path.join(songs_dir, song_file_name + \".mp3\")"
   ],
   "metadata": {
    "id": "A2PY9iuau2_4"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extracting the guitar track from the uploaded song. Let's check this feature on the Have You Ever Seen The Rain by Creedence Clearwater Revival which has a distinctive guitar track.\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "REPXTaEau2_5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "Audio(song_path)\n"
   ],
   "metadata": {
    "id": "FLg2dJ-xu2_5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Spleeter will divide the song into 4 separate tracks: bass, drums, vocals and other. The guitar will be contained in \"other\"."
   ],
   "metadata": {
    "collapsed": false,
    "id": "UXG6YViZu2_5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "command = f\"spleeter separate -p spleeter:4stems -o {separated_songs_dir} {song_path}\"\n",
    "subprocess.run(command, shell=True)"
   ],
   "metadata": {
    "id": "pbKjiheMu2_5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "id": "20fDQaLnu2_5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "guitar_track_path = os.path.join(separated_songs_dir, song_file_name, \"other.mp3\")\n",
    "\n",
    "Audio(guitar_track_path)\n"
   ],
   "metadata": {
    "id": "I25XBUP8u2_5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generating spectrograms from the extracted track\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "QXqSCBagu2_5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_dir = Path(os.path.join(\"data\", \"extracted\", song_file_name))"
   ],
   "metadata": {
    "id": "lercTZzau2_6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following function will process each segment of a track and convert it to a spectrogram. The spectrogram will be returned to be used as input to CNN and also will be saved to disk so that if we want to process the same track again, we could use existing data."
   ],
   "metadata": {
    "collapsed": false,
    "id": "2gaZcaQEu2_6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following function detects beat length and generates a spectrogram for each beat. Currently, it returns an array of spectrograms. Later on, predictions of a chord with CNN will be plugged in and it will return a list of predictions."
   ],
   "metadata": {
    "collapsed": false,
    "id": "e569jcHEu2_6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "def process_track(audio_path, output_dir):\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
    "\n",
    "    print(f\"Track tempo is {tempo}\")\n",
    "    beat_duration = 60.0 / tempo  # Duration of a beat in seconds\n",
    "\n",
    "    print(f\"Beat duration is {beat_duration} seconds\")\n",
    "\n",
    "    # Calculate the number of samples per beat\n",
    "    samples_per_beat = int(beat_duration * sr)\n",
    "\n",
    "    print(f\"Samples per beat: {samples_per_beat}\")\n",
    "\n",
    "    spectrograms = []\n",
    "    for i in range(0, len(y), samples_per_beat):\n",
    "        end_frame = i + samples_per_beat\n",
    "        if end_frame > len(y):\n",
    "            end_frame = len(y)  # Adjust the end frame for the last segment\n",
    "\n",
    "        segment = y[i:end_frame]\n",
    "        filename = f'spectrogram_{i // samples_per_beat}.jpg'\n",
    "        print(f\"Create spectrogram {filename}\")\n",
    "        spectrogram = create_spectrogram(segment, sr, os.path.join(output_dir, filename))\n",
    "        spectrograms.append(spectrogram)\n",
    "\n",
    "    return spectrograms\n"
   ],
   "metadata": {
    "id": "kEvvfu7lu2_6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To keep things simple, we will consider that the track already processed if the directory with the song name already exists. We can implement more sophisticated and robust checks like file hash sum later."
   ],
   "metadata": {
    "collapsed": false,
    "id": "LWoGfR9Gu2_6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    if os.path.exists(output_dir):\n",
    "        print(\"The track is already processed\", output_dir)\n",
    "    else:\n",
    "        os.makedirs(output_dir)\n",
    "        print(\"Processing track\", output_dir)\n",
    "        spectrograms = process_track(guitar_track_path, output_dir)\n",
    "\n",
    "        print(spectrograms[:10])\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    os.removedirs(output_dir)\n"
   ],
   "metadata": {
    "id": "RDxB-WYXu3AE"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's visualize an example of generated spectrogram"
   ],
   "metadata": {
    "collapsed": false,
    "id": "G_ypZyhxu3AE"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "files = os.listdir(output_dir)\n",
    "image_path = os.path.join(output_dir, files[50])\n",
    "\n",
    "print(f\"Displaying the image at {image_path}\")\n",
    "\n",
    "img = mpimg.imread(image_path)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "id": "LgWhiYrlu3AE"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fine tuning ResNet50\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "epSQxduXu3AF"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n"
   ],
   "metadata": {
    "id": "lY-oklXlu3AF"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the model and enable the last layer for fine tuning. If we have a saved version of the model, we can load its state."
   ],
   "metadata": {
    "collapsed": false,
    "id": "Q4KREqLGu3AF"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "# use GPU where available\n",
    "device = \"mps\" if getattr(torch, 'has_mps', False) \\\n",
    "    else \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_model(saved_model_path=None):\n",
    "  num_classes = 8 # we have 8 chords in our dataset\n",
    "  model = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "  # replacing the last layer for fine tuning\n",
    "  num_features = model.fc.in_features\n",
    "  model.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "\n",
    "  if saved_model_path:\n",
    "    model.load_state_dict(torch.load(saved_model_path, map_location=torch.device(device)))\n",
    "\n",
    "  return model.to(device)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j5nQPcFiu3AF",
    "outputId": "e817cac0-3c9d-4fa5-f232-4266aa8e0140"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = load_model()"
   ],
   "metadata": {
    "id": "YLLN_7Mz7t8N"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Categorical mapping of chords so that we could use them as numerical indexes in the neural network but also could decipher it back to actual labels for representation."
   ],
   "metadata": {
    "id": "PhvMWvY-75Vf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "chord_labels = [\"Am\", \"Bb\", \"Bdim\", \"C\", \"Dm\", \"Em\", \"F\", \"G\"]\n",
    "label_to_idx = {\n",
    "    \"Am\": 0,\n",
    "    \"Bb\": 1,\n",
    "    \"Bdim\": 2,\n",
    "    \"C\": 3,\n",
    "    \"Dm\": 4,\n",
    "    \"Em\": 5,\n",
    "    \"F\": 6,\n",
    "    \"G\": 7,\n",
    "}"
   ],
   "metadata": {
    "id": "yEBSo6Tl8CHp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define our dataset\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "XWwNPmQVu3AF"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, include_labels=True, image_mode=\"RGB\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the subdirectories for each label.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        self.label_to_idx = label_to_idx\n",
    "        self.include_labels = include_labels\n",
    "        self.image_mode = image_mode\n",
    "\n",
    "        # Iterate over each subdirectory in root_dir\n",
    "        for label_dir in os.listdir(root_dir):\n",
    "            label_dir_full_path = os.path.join(root_dir, label_dir)\n",
    "            if os.path.isdir(label_dir_full_path):\n",
    "\n",
    "                # Iterate over each file in the subdirectory\n",
    "                for file in os.listdir(label_dir_full_path):\n",
    "                    file_full_path = os.path.join(label_dir_full_path, file)\n",
    "                    if os.path.isfile(file_full_path):\n",
    "                        # Append the file path and its label to the samples list\n",
    "                        self.samples.append((file_full_path, self.label_to_idx[label_dir]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label_idx = self.samples[idx]\n",
    "        img = Image.open(img_path).convert(self.image_mode)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.include_labels:\n",
    "            return img, label_idx\n",
    "\n",
    "        return img\n"
   ],
   "metadata": {
    "id": "Eio5-74xu3AF"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The function to resize the images to the size expected by ResNet (224x224). As the original images are not square, it also pads them with blank space to keep the proportions as they are crucial for a time representation like the spectrogram."
   ],
   "metadata": {
    "collapsed": false,
    "id": "70rxfFMbu3AG"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "\n",
    "def resize_and_pad(spectrogram, target_size=(224, 224)):\n",
    "    # Calculate the resize ratio and resize the spectrogram\n",
    "    ratio = min(target_size[0] / spectrogram.width, target_size[1] / spectrogram.height)\n",
    "    new_size = (int(spectrogram.width * ratio), int(spectrogram.height * ratio))\n",
    "    spectrogram = spectrogram.resize(new_size, Image.LANCZOS)\n",
    "\n",
    "    # Calculate padding\n",
    "    delta_width = target_size[0] - new_size[0]\n",
    "    delta_height = target_size[1] - new_size[1]\n",
    "    padding = (delta_width // 2, delta_height // 2, delta_width - (delta_width // 2), delta_height - (delta_height // 2))\n",
    "\n",
    "    # Add padding\n",
    "    return ImageOps.expand(spectrogram, padding)\n"
   ],
   "metadata": {
    "id": "U_ikXvRGu3AG"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A function to calculate mean and standard deviation for normalization.\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "odH3Zu_7u3AG"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "def calculate_mean_std(loader):\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    total_images_count = 0\n",
    "    for images, _ in loader:\n",
    "        batch_samples = images.size(0)\n",
    "        channels = images.size(1)\n",
    "        images = images.view(batch_samples, channels, -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "        total_images_count += batch_samples\n",
    "\n",
    "    mean /= total_images_count\n",
    "    std /= total_images_count\n",
    "\n",
    "    return mean, std\n"
   ],
   "metadata": {
    "id": "aTK5WfyPu3AG"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transform the data to the input format expected by the network. This step without the normalization as we first need to calculate the mean and standard deviation.\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "9COMnosNu3AG"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "pre_transform = transforms.Compose([\n",
    "    transforms.Lambda(resize_and_pad),\n",
    "    transforms.ToTensor(),\n",
    "])\n"
   ],
   "metadata": {
    "id": "UBqZ21SIu3AG"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following operation is to calculate mean and std across the training dataset for normalization. It is an expensive operation so this cell is disabled and the following cell contains those values received from a previous calculation."
   ],
   "metadata": {
    "collapsed": false,
    "id": "b31QXH_pu3AG"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "#| code: true\n",
    "#| output: false\n",
    "#} eval: false\n",
    "\n",
    "pre_dataset = SpectrogramDataset(root_dir=preprocessed_train_data_dir, transform=pre_transform)\n",
    "\n",
    "pre_loader = DataLoader(pre_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
    "\n",
    "mean, std = calculate_mean_std(pre_loader)\n",
    "print(mean, std)\n"
   ],
   "metadata": {
    "id": "bQSBsSiHu3AG"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mean = [0.1923, 0.0816, 0.1362]\n",
    "std = [0.3121, 0.1567, 0.1977]"
   ],
   "metadata": {
    "id": "nPUjEcUau3AH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have all necessary information, we can fully transform the dataset."
   ],
   "metadata": {
    "collapsed": false,
    "id": "YRypMVXsu3AH"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(resize_and_pad),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "])"
   ],
   "metadata": {
    "id": "-UqW_-uPu3AH"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = SpectrogramDataset(root_dir=preprocessed_train_data_dir, transform=transform)\n",
    "len(dataset)"
   ],
   "metadata": {
    "id": "XAN024Amu3AH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create validation dataset"
   ],
   "metadata": {
    "collapsed": false,
    "id": "4nVG82ycu3AH"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, validation_dataset = random_split(dataset, [train_size, val_size])\n",
    "len(validation_dataset)"
   ],
   "metadata": {
    "id": "R_6E-KsTu3AH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following function is used for testing the model and returns current loss and accuracy. It is used both for hyperparameter tuning feedback and for testing the model on the test dataset."
   ],
   "metadata": {
    "collapsed": false,
    "id": "20YYxD0vu3AI"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "id": "m2Pjyf3du3AI"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n"
   ],
   "metadata": {
    "id": "xUxWMTtTu3AK"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_epochs = 18"
   ],
   "metadata": {
    "id": "dPKG7Ogfu3AI"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The model training loop. It uses checkpoints as the training is a long operation that might be interrupted and it that case we should be able to continue where we left off."
   ],
   "metadata": {
    "id": "kHQa9MBi9K0f"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from ray import train\n",
    "import tempfile\n",
    "from ray.train import Checkpoint\n",
    "\n",
    "def train_model(config):\n",
    "    start = 1\n",
    "    checkpoint = train.get_checkpoint()\n",
    "    if checkpoint:\n",
    "        print(\"Load from checkpoint\", checkpoint)\n",
    "        with checkpoint.as_directory() as checkpoint_dir:\n",
    "            checkpoint_dict = torch.load(os.path.join(checkpoint_dir, \"checkpoint.pt\"))\n",
    "            start = checkpoint_dict[\"epoch\"] + 1\n",
    "            model.load_state_dict(checkpoint_dict[\"model_state\"])\n",
    "\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    optimizer = getattr(torch.optim, config[\"optimizer\"])(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    val_loader = DataLoader(validation_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    num_epochs = config.get(\"epochs\", 7)\n",
    "\n",
    "    for epoch in range(start, num_epochs):\n",
    "      print(f\"Start epoch {epoch}\")\n",
    "      model.train()\n",
    "      running_loss = 0.0\n",
    "      for inputs, labels in train_loader:\n",
    "          inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          outputs = model(inputs)\n",
    "          loss = criterion(outputs, labels)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "          running_loss += loss.item()\n",
    "\n",
    "      # validation\n",
    "      val_loss, val_accuracy = evaluate_model(model, val_loader, criterion)\n",
    "      print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "      with tempfile.TemporaryDirectory() as tempdir:\n",
    "        print(\"Save to checkpoint\", os.path.join(tempdir, \"checkpoint.pt\"))\n",
    "        torch.save(\n",
    "            {\"epoch\": epoch, \"model_state\": model.state_dict()},\n",
    "            os.path.join(tempdir, \"checkpoint.pt\"),\n",
    "        )\n",
    "\n",
    "        checkpoint=Checkpoint.from_directory(tempdir)\n",
    "        # send data to Ray Tune at each epoch to allow the scheduler to cancel inefficient experiments early\n",
    "        train.report({\"val_loss\": val_loss, \"accuracy\": val_accuracy})\n"
   ],
   "metadata": {
    "id": "sBRUqnQWu3AI"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameter tuning"
   ],
   "metadata": {
    "collapsed": false,
    "id": "PTVE-XPRu3AI"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here I use a Ray Tune scheduler to be able to cut off inefficient experiments early and improve the efficiency of hyperparameter tuning"
   ],
   "metadata": {
    "collapsed": false,
    "id": "qRcjYnyiu3AJ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    max_t = max_epochs,\n",
    "    grace_period=1,\n",
    "    reduction_factor=2,\n",
    ")\n"
   ],
   "metadata": {
    "id": "jslQRUXxu3AJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "These are hyperparameters that we are going to choose."
   ],
   "metadata": {
    "id": "1OFK0lfb9hl-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray.tune import conditional\n",
    "\n",
    "# TODO: run a short experiment for optimizer choice?\n",
    "config = {\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"batch_size\": tune.choice([16, 32, 64]),\n",
    "    \"epochs\": tune.choice([7, 10, 12, 15, 18]),\n",
    "    \"optimizer\": tune.choice([\"Adam\", \"SGD\"]),\n",
    "    \"momentum\": conditional(\n",
    "        lambda spec: spec.config.optimizer == \"SGD\",\n",
    "        tune.uniform(0.8, 0.99),\n",
    "    ),\n",
    "}"
   ],
   "metadata": {
    "id": "QKou6L0Au3AJ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import ray\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True, local_mode=True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 76
    },
    "id": "II7CAXI-u3AJ",
    "outputId": "83510674-507e-4285-cd54-f541cd58130f"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_best_params(config, train_model, scheduler, num_samples=10):\n",
    "    analysis = tune.run(\n",
    "        train_model,\n",
    "        config=config,\n",
    "        num_samples=num_samples,  # Number of times to sample from the hyperparameter space\n",
    "        resources_per_trial={\"cpu\": 2, \"gpu\": 2560},  # Resources per trial\n",
    "        scheduler=scheduler,\n",
    "        resume=\"AUTO\"\n",
    "    )\n",
    "\n",
    "    return analysis.get_best_config(metric=\"accuracy\", mode=\"max\")"
   ],
   "metadata": {
    "id": "aC58WU_xu3AJ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "742acc89-f69d-4031-9587-7b3dadaaf558"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this is the most successful config received by running hyperparameter tuning on Google Colab\n",
    "best_config = {\"lr\": 0.002610838003776132, \"batch_size\": 32, \"epochs\": 12}"
   ],
   "metadata": {
    "id": "UTR93zqGu3AJ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1ef0d6f4-14fe-41da-a8ad-d058c9395964"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run this cell to get current best config result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_config = get_best_params(config, train_model, scheduler, 30)\n",
    "print(\"Best config: \", best_config)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training the model with optimal hyperparameters\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "bOkg4VSYu3AK"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_model(best_config)"
   ],
   "metadata": {
    "id": "f8p1bI-bu3AK",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "fe5e1f53-d1da-4aac-f1d0-ba31da581894"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "torch.save(model.state_dict(), os.path.join(pretrained_models_dir, \"chord_classifier.pth\"))"
   ],
   "metadata": {
    "id": "30vKzQU9ljnb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transfer Learning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a custom neural network which will accept features from pre-trained and fine tuned ResNet50 to make final classifications. This will allow for more flexibility in tuning the architecture for this particular task."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = load_model(os.path.join(pretrained_models_dir, \"chord_classifier_actual.pth\"))\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ChordClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "        # accept features from ResNet50 (size 2048)\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        # add non linearity\n",
    "        self.relu = nn.ReLU()\n",
    "        # accept 512-dimensional features from the previous fully connected layer\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        # output predictions as probabilities\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.softmax(out)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Replace the last ResNet50 layer with an identity function that will just return features without changes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.fc = nn.Identity()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extract features using pre-trained and fine-tuned ResNet50 and save them into a HDF5 file on disk for training the classifier."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "dataset = SpectrogramDataset(root_dir=preprocessed_train_data_dir, transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "with h5py.File('features_labels.h5', 'w') as h5f:\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs)\n",
    "\n",
    "        # Convert to numpy and write to disk\n",
    "        features_batch = output.cpu().detach().numpy()\n",
    "        labels_batch = labels.numpy()\n",
    "\n",
    "        # Create datasets for the first batch and then append for subsequent batches\n",
    "        if i == 0:\n",
    "            h5f.create_dataset('features', data=features_batch, maxshape=(None, features_batch.shape[1]), chunks=True)\n",
    "            h5f.create_dataset('labels', data=labels_batch, maxshape=(None,), chunks=True)\n",
    "        else:\n",
    "            h5f['features'].resize((h5f['features'].shape[0] + features_batch.shape[0]), axis=0)\n",
    "            h5f['features'][-features_batch.shape[0]:] = features_batch\n",
    "            h5f['labels'].resize((h5f['labels'].shape[0] + labels_batch.shape[0]), axis=0)\n",
    "            h5f['labels'][-labels_batch.shape[0]:] = labels_batch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Load features and labels\n",
    "with h5py.File('features_labels.h5', 'r') as hf:\n",
    "    features = hf['features'][:]\n",
    "    labels = hf['labels'][:]\n",
    "\n",
    "features = torch.tensor(features, dtype=torch.float)\n",
    "labels = torch.tensor(labels, dtype=torch.long)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Split the data into training and validation sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features_train, features_val, labels_train, labels_val = train_test_split(\n",
    "    features, labels, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(features_train, labels_train)\n",
    "val_dataset = TensorDataset(features_val, labels_val)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train and validate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classifier_model = ChordClassifier(input_size=features.shape[1], num_classes=len(torch.unique(labels)))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from ray import train\n",
    "\n",
    "def train_classifier_model(config):\n",
    "    num_epochs = config['epochs']\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    val_loader = DataLoader(validation_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Start epoch {epoch}\")\n",
    "        classifier_model.train()\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = classifier_model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation loop\n",
    "        classifier_model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                output = classifier_model(data)\n",
    "                val_loss += criterion(output, target).item()\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        val_loss, val_accuracy = evaluate_model(classifier_model, val_loader, criterion)\n",
    "        train.report({\"val_loss\": val_loss, \"accuracy\": val_accuracy})\n",
    "        print(f'Epoch {epoch+1}, Val Loss: {val_loss:.4f}, Val Accuracy: {correct / len(val_loader.dataset):.4f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hyperparameters tuning. The classifier is a simpler network than ResNet50 and trains much faster so we can afford more relaxed scheduler settings and deeper exploration of the hyperparameters space."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    max_t = 200,\n",
    "    grace_period=3,\n",
    "    reduction_factor=2,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "\n",
    "config = {\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"batch_size\": tune.choice([8, 16, 32, 64]),\n",
    "    \"epochs\": tune.choice([50, 100, 150, 200, 250, 300])\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_config = get_best_params(config, train_classifier_model, scheduler, 100)\n",
    "print(\"Best config: \", best_config)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_classifier_model()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing the model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_dataset = SpectrogramDataset(root_dir=preprocessed_test_data_dir, transform=transform, include_labels=False)\n",
    "len(test_dataset)"
   ],
   "metadata": {
    "id": "k7b0PZJDu3AK",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1ac1be1b-0939-47bd-fccd-82341145dc54"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_loader = DataLoader(train_dataset, batch_size=best_config[\"batch_size\"], shuffle=True)"
   ],
   "metadata": {
    "id": "xEOLHtDDu3AL"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "precision = precision_score(all_targets, all_predictions, average='macro')\n",
    "recall = recall_score(all_targets, all_predictions, average='macro')\n",
    "f1 = f1_score(all_targets, all_predictions, average='macro')\n",
    "\n",
    "conf_matrix = confusion_matrix(all_targets, all_predictions, labels=range(len(chord_labels)))\n",
    "\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ],
   "metadata": {
    "id": "PDGFpUk0u3AL",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "959a18d1-862e-4fb7-f1ce-21db6ac75d3f"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Metrics received on test dataset:\n",
    "\n",
    "Precision: 0.9044754119873304\n",
    "\n",
    "Recall: 0.8930117832821793\n",
    "\n",
    "F1 Score: 0.8953096711745837"
   ],
   "metadata": {
    "collapsed": false,
    "id": "AmMORCKTu3AL"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "conf_matrix"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oV0bycDNEnm8",
    "outputId": "2852a8b6-3219-4298-f55b-831d239b979f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='g', xticklabels=chord_labels, yticklabels=chord_labels)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "id": "mB0dq8E7u3AL",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3ea0a585-ae43-4b6a-8878-513a52f4a332"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "eoy3YSI8u3AL"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def load_spectrogram(file_path):\n",
    "    image = Image.open(file_path).convert('RGB')\n",
    "    return transform(image)"
   ],
   "metadata": {
    "id": "TMlaZwv9u3AL"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Switch to evaluation mode and convert logits to probabilities using softmax\n"
   ],
   "metadata": {
    "id": "AmN4yNA492Qh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define the softmax function\n",
    "softmax = torch.nn.Softmax(dim=1)"
   ],
   "metadata": {
    "id": "y7dabTIL92sR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sort spectrograms by file name as it contains its order number. We need to have it ordered to be able to output it aligned with track timeline."
   ],
   "metadata": {
    "id": "Y6ajig6M99FI"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_dir"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# List all jpg files in the output directory\n",
    "file_names = [file for file in os.listdir(output_dir) if file.endswith('.jpg')]\n",
    "\n",
    "# Sort the file names (natural sort or simple alphanumeric sort)\n",
    "file_names.sort(key=lambda f: int(f.split('_')[-1].split('.')[0]))\n",
    "\n",
    "spectrograms = [load_spectrogram(os.path.join(output_dir, file)) for file in file_names]"
   ],
   "metadata": {
    "id": "ZnouPWsO99qI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Make predictions for each spectrogram. For each prediction we get top N results. We also map them to labels for output."
   ],
   "metadata": {
    "id": "jTse-xad-e5h"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "index_to_label = {v: k for k,v in label_to_idx.items()}\n",
    "\n",
    "N = 3  # Number of top predictions\n",
    "\n",
    "top_labels_all = []\n",
    "top_confidences_all = []\n",
    "\n",
    "for spectrogram in spectrograms:\n",
    "    # make sure spectrograms are on the same device as the model, otherwise there will be input type mismatch\n",
    "    spectrogram = spectrogram.to(device)\n",
    "    # Apply the model and get the prediction\n",
    "    output = model(spectrogram.unsqueeze(0))  # Add batch dimension\n",
    "    probabilities = softmax(output)\n",
    "\n",
    "    # Get the top N predictions\n",
    "    top_probs, top_preds = torch.topk(probabilities, N, dim=1)\n",
    "\n",
    "    # Convert to labels and confidences\n",
    "    top_labels = [index_to_label[pred.item()] for pred in top_preds[0]]\n",
    "    top_confidences = [prob.item() for prob in top_probs[0]]\n",
    "\n",
    "    top_labels_all.append(top_labels)\n",
    "    top_confidences_all.append(top_confidences)"
   ],
   "metadata": {
    "id": "qfmCyTJg-Ysq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Output"
   ],
   "metadata": {
    "id": "tJ1KM4F7-3hP"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* For each beat, we get top N predictions\n",
    "* Low confidence predictions with threshold lower than 0.3 will be discarded\n",
    "* We will consider two adjacents beats at once. First, chords that appear in both beats predictions will be chosen. If there are several such chords, then the chord with the highest average confidence will be chosen."
   ],
   "metadata": {
    "id": "b-hMILEE-80c"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for file, labels, confidences in zip(file_names[:20], top_labels_all[:20], top_confidences_all[:20]):\n",
    "    print(f\"{file}:\")\n",
    "    for label, confidence in zip(labels, confidences):\n",
    "        print(f\"  {label}: {confidence:.4f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I35BkV0O-0hb",
    "outputId": "8891c3ad-9770-474a-d91c-11b5f220e243"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def map_label_to_confidence(labels, confidences):\n",
    "    return {label: confidence for label, confidence in zip(labels, confidences)}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_with_max_confidence(common_labels, prev_labels_to_confidence, labels_to_confidence):\n",
    "    highest_conf_label = None\n",
    "    highest_confidence = 0\n",
    "\n",
    "    for label in common_labels:\n",
    "        avg_conf = (prev_labels_to_confidence.get(label, 0) + labels_to_confidence.get(label, 0)) / 2\n",
    "        if avg_conf > highest_confidence:\n",
    "            highest_confidence = avg_conf\n",
    "            highest_conf_label = label\n",
    "\n",
    "    return highest_conf_label, highest_confidence"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def process_chord_predictions(top_labels_all, top_confidences_all, confidence_threshold=0.3):\n",
    "    chord_predictions = []\n",
    "\n",
    "    for i in range(1, len(top_labels_all), 2):\n",
    "        prev_labels = top_labels_all[i - 1]\n",
    "        labels = top_labels_all[i]\n",
    "        prev_confidences = top_confidences_all[i - 1]\n",
    "        confidences = top_confidences_all[i]\n",
    "\n",
    "        prev_labels_to_confidence = map_label_to_confidence(prev_labels, prev_confidences)\n",
    "        labels_to_confidence = map_label_to_confidence(labels, confidences)\n",
    "\n",
    "        # Filter labels by confidence threshold\n",
    "        prev_filtered = {label: conf for label, conf in prev_labels_to_confidence.items() if conf >= confidence_threshold}\n",
    "        filtered = {label: conf for label, conf in labels_to_confidence.items() if conf >= confidence_threshold}\n",
    "\n",
    "        common_labels = set(prev_filtered.keys()).intersection(filtered.keys())\n",
    "\n",
    "        if common_labels:\n",
    "            chosen_label, chosen_confidence = get_with_max_confidence(common_labels, prev_labels_to_confidence, labels_to_confidence)\n",
    "        else:\n",
    "            # Combine and sort all labels by confidence, regardless of threshold, if no common labels meet the threshold\n",
    "            all_labels_confidences = list(prev_labels_to_confidence.items()) + list(labels_to_confidence.items())\n",
    "            chosen_label, chosen_confidence = max(all_labels_confidences, key=lambda x: x[1])\n",
    "\n",
    "        chord_predictions.append((chosen_label, chosen_confidence))\n",
    "\n",
    "    return chord_predictions"
   ],
   "metadata": {
    "id": "ezVrdXhw-5vo"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chord_predictions = process_chord_predictions(top_labels_all, top_confidences_all)\n",
    "print(chord_predictions)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Combine predictions into a chord sequence."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "combined_predictions = []\n",
    "\n",
    "# Initialize the previous chord variable with None\n",
    "prev_chord = None\n",
    "\n",
    "# Iterate through each prediction\n",
    "for chord, confidence in chord_predictions:\n",
    "    # Check if the current chord is different from the previous chord\n",
    "    if chord != prev_chord:\n",
    "        # If it's different, append it to the combined list\n",
    "        combined_predictions.append(chord)\n",
    "        # Update the previous chord\n",
    "        prev_chord = chord"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "def display_chords(chord_predictions):\n",
    "    # Define a color scheme for each chord\n",
    "    chord_colors = {\n",
    "        \"Am\": \"#FFD700\",  # Gold\n",
    "        \"Bb\": \"#FF4500\",  # OrangeRed\n",
    "        \"Bdim\": \"#1E90FF\",  # DodgerBlue\n",
    "        \"C\": \"#32CD32\",  # LimeGreen\n",
    "        \"Dm\": \"#BA55D3\",  # MediumOrchid\n",
    "        \"Em\": \"#FF69B4\",  # HotPink\n",
    "        \"F\": \"#00CED1\",  # DarkTurquoise\n",
    "        \"G\": \"#FFA500\",  # Orange\n",
    "    }\n",
    "\n",
    "    # Start the HTML string for output\n",
    "    output = \"<div style='display: flex; flex-wrap: wrap;'>\"\n",
    "\n",
    "    # Counter to keep track of chords per line\n",
    "    chords_per_line = 0\n",
    "\n",
    "    for i, chord in enumerate(chord_predictions, start=1):\n",
    "        # Get the color for the current chord\n",
    "        color = chord_colors.get(chord, \"grey\")  # Default to grey if chord not found\n",
    "\n",
    "        # Create a div for the chord with the specific background color and white text for contrast\n",
    "        output += f\"<div style='color: white; margin: 5px; padding: 10px; background-color: {color}; width: 100px; text-align: center;'>{chord}</div>\"\n",
    "\n",
    "        # Increment the counter\n",
    "        chords_per_line += 1\n",
    "\n",
    "        # Check if we've reached 4 chords or the end of the list, then reset counter and add a line break\n",
    "        if chords_per_line == 4 or i == len(chord_predictions):\n",
    "            output += \"<div style='flex-basis: 100%; height: 0;'></div>\"  # This creates a line break in flexbox\n",
    "            chords_per_line = 0\n",
    "\n",
    "    # Close the HTML string\n",
    "    output += \"</div>\"\n",
    "\n",
    "    # Display the HTML in the Jupyter Notebook\n",
    "    display(HTML(output))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display_chords(combined_predictions)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "V100"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
